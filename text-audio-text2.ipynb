{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPqFVCqpeZ9Nd66Nrw7HFVM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!python -m spacy download en_core_web_sm"],"metadata":{"id":"jqt6ZKloU00E"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"75nMr6pVSD8W"},"outputs":[],"source":["!pip install transformers torch librosa soundfile spacy pydub gradio tensorflow tensorflow_hub speechbrain torchaudio"]},{"cell_type":"code","source":["# import os\n","# os.kill(os.getpid(), 9)"],"metadata":{"id":"_VD0kngiSILK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/tensorflow/models/master/research/audioset/yamnet/yamnet_class_map.csv\n","!pip install openai-whisper\n","!pip install git+https://github.com/openai/whisper.git\n","!pip install git+https://github.com/speechbrain/speechbrain.git@develop\n","# !pip install torch"],"metadata":{"id":"mff1naVBSJq7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import whisper\n","\n","# Load the Whisper model\n","whisper_model = whisper.load_model(\"base\")\n","\n","def get_text_from_audio(audio_file):\n","    file_path = convert_to_wav(audio_file)  # Convert to wav for Whisper compatibility\n","    result = whisper_model.transcribe(file_path)\n","    transcription = result['text']\n","    return transcription"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n8GEsYYPSLtZ","executionInfo":{"status":"ok","timestamp":1729765267463,"user_tz":-330,"elapsed":13650,"user":{"displayName":"Prathamesh Nimkar","userId":"03839392730929117675"}},"outputId":"6eccfa31-c826-4615-ffbd-f8812278b825"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 139M/139M [00:02<00:00, 55.0MiB/s]\n","/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(fp, map_location=device)\n"]}]},{"cell_type":"code","source":["emotion_mapping = {\n","    \"LABEL_0\": \"sadness\",\n","    \"LABEL_1\": \"joy\",\n","    \"LABEL_2\": \"love\",\n","    \"LABEL_3\": \"anger\",\n","    \"LABEL_4\": \"fear\",\n","    \"LABEL_5\": \"surprise\"\n","}"],"metadata":{"id":"bN3sJBgU0u5l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gradio as gr\n","import torch\n","import librosa\n","import soundfile as sf\n","import torchaudio\n","from transformers import pipeline\n","import whisper\n","import spacy\n","import numpy as np\n","from pydub import AudioSegment\n","\n","# Load models\n","whisper_model = whisper.load_model(\"base\")\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","summarizer = pipeline(\"summarization\", model=\"t5-base\", device=0)\n","sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", device=0)\n","emotion_recognizer = pipeline(\"text-classification\", model=\"mrm8488/t5-base-finetuned-emotion\", device=0)\n","\n","def convert_to_wav(file_path):\n","    if not file_path.endswith(\".wav\"):\n","        audio = AudioSegment.from_file(file_path)\n","        wav_path = \"converted_temp.wav\"\n","        audio.export(wav_path, format=\"wav\")\n","        return wav_path\n","    return file_path\n","\n","def split_audio_into_chunks(file_path, chunk_length=30):\n","    audio = AudioSegment.from_wav(file_path)\n","    chunk_length_ms = chunk_length * 1000  # Convert to milliseconds\n","    chunks = [audio[i:i + chunk_length_ms] for i in range(0, len(audio), chunk_length_ms)]\n","    return chunks\n","\n","def transcribe_chunk(chunk):\n","    temp_path = \"temp_chunk.wav\"\n","    chunk.export(temp_path, format=\"wav\")\n","    result = whisper_model.transcribe(temp_path)\n","    return result['text']\n","\n","def get_combined_transcription(audio_file, chunk_length=30):\n","    \"\"\"Split the audio file into chunks and transcribe each chunk.\"\"\"\n","    file_path = convert_to_wav(audio_file)\n","    chunks = split_audio_into_chunks(file_path, chunk_length=chunk_length)\n","\n","    combined_transcription = \"\"\n","    for chunk in chunks:\n","        transcription = transcribe_chunk(chunk)\n","        combined_transcription += transcription + \" \"\n","\n","    return combined_transcription.strip()\n","\n","def classify_text_emotion(text):\n","    result = emotion_recognizer(text)\n","    raw_emotion = result[0][\"label\"]\n","\n","    # Map the raw emotion label (e.g., \"LABEL_1\") to a human-readable emotion\n","    emotion = emotion_mapping.get(raw_emotion, \"unknown emotion\")\n","\n","    return emotion\n","\n","def generate_descriptive_insights(transcription, audio_emotion):\n","    # Truncate the transcription to 512 tokens for sentiment analysis\n","    truncated_transcription = transcription[:512]\n","\n","    # Determine the appropriate max_length based on the transcription length\n","    max_length = min(len(truncated_transcription.split()), 80)\n","    min_length = min(len(truncated_transcription.split()), 25)\n","\n","    # Step 1: Summarize the content\n","    summary = summarizer(truncated_transcription, max_length=max_length, min_length=min_length, do_sample=False)\n","    summarized_text = summary[0]['summary_text']\n","\n","    # Step 2: Analyze the sentiment\n","    sentiment = sentiment_analyzer(truncated_transcription)\n","    mood = sentiment[0]['label'].lower()\n","\n","    # Step 3: Generate descriptive insights considering both sentiment and emotion\n","    mood_description = f\"Based on the transcription, the speaker seems to have a {mood} mood. \"\n","    mood_description += f\"The text analysis indicates the speaker's emotion is {audio_emotion}. \"\n","    mood_description += f\"Topic discussed: {summarized_text}\"\n","\n","    return mood_description\n","\n","def process_audio_and_text(audio, text=None):\n","    \"\"\"Process the audio and optional text input, then derive insights.\"\"\"\n","    # Step 1: Transcribe the audio file in chunks and combine the transcriptions\n","    transcription = get_combined_transcription(audio)\n","\n","    # Step 2: Perform text-based emotion analysis\n","    audio_emotion = classify_text_emotion(transcription)\n","\n","    # Step 3: Handle optional text input (if provided)\n","    if text and text.strip():\n","        doc = nlp(text)\n","        if \"mood\" in text.lower() or \"insight\" in text.lower() or \"saying\" in text.lower() or \"implying\" in text.lower():\n","            # Generate descriptive insights based on transcription and audio emotion\n","            descriptive_insights = generate_descriptive_insights(transcription, audio_emotion)\n","            return transcription, {\"Descriptive Insight\": descriptive_insights}\n","        elif \"genre\" in text.lower():\n","            return transcription, {\"Genre\": \"Music\"}  # Simplified for now\n","        else:\n","            text_insights = derive_insights_from_text(transcription)\n","            return transcription, text_insights\n","\n","    # Default behavior when no text is provided\n","    insights = derive_insights_from_text(transcription)\n","    return transcription, {\"Audio Emotion\": audio_emotion, \"Insights\": insights}\n","\n","def derive_insights_from_text(text):\n","    \"\"\"Derive insights using text analysis (entity extraction, keywords).\"\"\"\n","    doc = nlp(text)\n","    entities = [(ent.text, ent.label_) for ent in doc.ents]\n","    keywords = [chunk.text for chunk in doc.noun_chunks]\n","\n","    insights = {\n","        \"Extracted Entities\": entities,\n","        \"Key Phrases\": keywords\n","    }\n","\n","    return insights\n","\n","# Gradio Interface\n","with gr.Blocks() as demo:\n","    with gr.Row():\n","        audio_input = gr.Audio(type=\"filepath\", label=\"Upload Audio File\")\n","        optional_text = gr.Textbox(lines=2, label=\"Optional Text Input (e.g., 'What is the mood?' or 'What is the genre? or the insights')\")\n","\n","    with gr.Row():\n","        audio_transcription = gr.Textbox(label=\"Transcription\", placeholder=\"Audio transcription will appear here.\")\n","        output_insights = gr.JSON(label=\"Derived Insights or Answer\")\n","\n","    audio_submit = gr.Button(\"Process Audio and Text\")\n","\n","    audio_submit.click(fn=process_audio_and_text, inputs=[audio_input, optional_text], outputs=[audio_transcription, output_insights])\n","\n","demo.launch(debug=True)"],"metadata":{"id":"Wl_R4tysSQWM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cm5SyUft0euW"},"execution_count":null,"outputs":[]}]}